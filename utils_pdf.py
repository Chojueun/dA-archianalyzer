"""
í†µí•© PDF ì²˜ë¦¬ ëª¨ë“ˆ
- PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
- PDF ì €ì¥ ë° ê²€ìƒ‰
- PDF ìš”ì•½ ì •ë³´ ê´€ë¦¬
"""

import streamlit as st
import fitz  # PyMuPDF
import re
from typing import List

# ì „ì—­ ë³€ìˆ˜ (ë²¡í„° ì‹œìŠ¤í…œìš©)
embedder = None
collection = None
chroma_client = None

def initialize_vector_system():
    """ë²¡í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ê°„ë‹¨ ê²€ìƒ‰ë§Œ ì‚¬ìš©"""
    global embedder, collection, chroma_client
    
    # ê³ ê¸‰ ë²¡í„° ì‹œìŠ¤í…œ ì™„ì „ ë¹„í™œì„±í™”
    embedder = None
    collection = None
    chroma_client = None
    
    # ë©”ì‹œì§€ ì œê±° - ì¡°ìš©íˆ True ë°˜í™˜
    return True

def extract_text_from_pdf(pdf_input, input_type="path") -> str:
    """
    í†µí•©ëœ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ - êµ¬ì¡°í™”ëœ í˜•ì‹ ì œê±°
    
    Args:
        pdf_input: PDF íŒŒì¼ ê²½ë¡œ(str) ë˜ëŠ” ë°”ì´íŠ¸(bytes)
        input_type: "path" ë˜ëŠ” "bytes"
    
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (êµ¬ì¡°í™”ëœ í˜•ì‹ ì œê±°)
    """
    try:
        if input_type == "path":
            # íŒŒì¼ ê²½ë¡œë¡œë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            doc = fitz.open(pdf_input)
            text = ""
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text += page.get_text() + "\n"
            
            doc.close()
            
        elif input_type == "bytes":
            # ë°”ì´íŠ¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            with fitz.open(stream=pdf_input, filetype="pdf") as doc:
                text = "\n".join([page.get_text() for page in doc])
        else:
            raise ValueError("input_type must be 'path' or 'bytes'")
        
        # ë””ë²„ê¹…: ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ í˜•ì‹ í™•ì¸
        if "**ìš”ì•½**:" in text or "**ì¸ì‚¬ì´íŠ¸**:" in text or "**ìƒì„¸ ë¶„ì„**:" in text:
            print("ğŸ” êµ¬ì¡°í™”ëœ í˜•ì‹ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬ ì¤‘...")
        
        # êµ¬ì¡°í™”ëœ í˜•ì‹ ì œê±° (ìš”ì•½:, ì¸ì‚¬ì´íŠ¸:, ìƒì„¸ ë¶„ì„: ë“±)
        cleaned_text = clean_structured_format(text)
        
        # ë””ë²„ê¹…: ì •ë¦¬ í›„ í…ìŠ¤íŠ¸ í™•ì¸
        if len(cleaned_text) != len(text):
            print(f"âœ… í…ìŠ¤íŠ¸ ì •ë¦¬ ì™„ë£Œ: {len(text)} â†’ {len(cleaned_text)} ë¬¸ì")
        
        return cleaned_text
            
    except Exception as e:
        st.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def clean_structured_format(text: str) -> str:
    """
    êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ì œê±°í•˜ê³  ì‹¤ì œ ë‚´ìš©ë§Œ ì¶”ì¶œ - ê°•í™”ëœ ë²„ì „
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
    
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    # ì œê±°í•  êµ¬ì¡°í™”ëœ í˜•ì‹ë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
    patterns_to_remove = [
        # ë§ˆí¬ë‹¤ìš´ í˜•ì‹
        r'\*\*ìš”ì•½\*\*:\s*\n?',
        r'\*\*ì¸ì‚¬ì´íŠ¸\*\*:\s*\n?', 
        r'\*\*ìƒì„¸ ë¶„ì„\*\*:\s*\n?',
        r'\*\*Summary\*\*:\s*\n?',
        r'\*\*Insight\*\*:\s*\n?',
        r'\*\*Detailed Analysis\*\*:\s*\n?',
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸ í˜•ì‹
        r'ìš”ì•½:\s*\n?',
        r'ì¸ì‚¬ì´íŠ¸:\s*\n?',
        r'ìƒì„¸ ë¶„ì„:\s*\n?',
        r'Summary:\s*\n?',
        r'Insight:\s*\n?',
        r'Detailed Analysis:\s*\n?',
        
        # ì¶”ê°€ íŒ¨í„´ë“¤
        r'^\s*\*\*ìš”ì•½\*\*:\s*$',
        r'^\s*\*\*ì¸ì‚¬ì´íŠ¸\*\*:\s*$',
        r'^\s*\*\*ìƒì„¸ ë¶„ì„\*\*:\s*$',
        r'^\s*ìš”ì•½:\s*$',
        r'^\s*ì¸ì‚¬ì´íŠ¸:\s*$',
        r'^\s*ìƒì„¸ ë¶„ì„:\s*$',
        
        # ë¹ˆ ì¤„ê³¼ í•¨ê»˜
        r'\n\s*\*\*ìš”ì•½\*\*:\s*\n',
        r'\n\s*\*\*ì¸ì‚¬ì´íŠ¸\*\*:\s*\n',
        r'\n\s*\*\*ìƒì„¸ ë¶„ì„\*\*:\s*\n',
        r'\n\s*ìš”ì•½:\s*\n',
        r'\n\s*ì¸ì‚¬ì´íŠ¸:\s*\n',
        r'\n\s*ìƒì„¸ ë¶„ì„:\s*\n'
    ]
    
    cleaned_text = text
    
    # ê° íŒ¨í„´ ì œê±°
    for pattern in patterns_to_remove:
        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE | re.MULTILINE)
    
    # ë¹ˆ ì„¹ì…˜ ì œê±° (ì œëª©ë§Œ ìˆê³  ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°)
    section_patterns = [
        r'\*\*ìš”ì•½\*\*:\s*\n\s*\n',
        r'\*\*ì¸ì‚¬ì´íŠ¸\*\*:\s*\n\s*\n',
        r'\*\*ìƒì„¸ ë¶„ì„\*\*:\s*\n\s*\n',
        r'ìš”ì•½:\s*\n\s*\n',
        r'ì¸ì‚¬ì´íŠ¸:\s*\n\s*\n',
        r'ìƒì„¸ ë¶„ì„:\s*\n\s*\n'
    ]
    
    for pattern in section_patterns:
        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE | re.MULTILINE)
    
    # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
    cleaned_text = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text

def save_pdf_chunks_to_chroma(pdf_path: str, pdf_id: str = "default") -> bool:
    """
    PDF ì²­í¬ë¥¼ ê°„ë‹¨ ì €ì¥ìœ¼ë¡œ ì²˜ë¦¬
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        pdf_id: PDF ì‹ë³„ì
    
    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    try:
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(pdf_path, "path")
        
        if not text:
            st.error("âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            return False
        
        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        if 'pdf_chunks' not in st.session_state:
            st.session_state.pdf_chunks = {}
        
        st.session_state.pdf_chunks[pdf_id] = text
        st.success(f"âœ… PDFê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        st.error(f"âŒ PDF ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

def search_pdf_chunks(query: str, pdf_id: str = "default", top_k: int = 3) -> str:
    """
    PDF ê²€ìƒ‰ í•¨ìˆ˜ - ê°„ë‹¨ ê²€ìƒ‰ë§Œ ì‚¬ìš©
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        pdf_id: PDF ì‹ë³„ì
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
    
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼
    """
    return fallback_to_simple_search(query, pdf_id, top_k)

def fallback_to_simple_search(query: str, pdf_id: str, top_k: int) -> str:
    """
    ê°„ë‹¨ ê²€ìƒ‰ - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        pdf_id: PDF ì‹ë³„ì
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
    
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        # PDF í…ìŠ¤íŠ¸ í™•ì¸
        if 'pdf_chunks' not in st.session_state or pdf_id not in st.session_state.pdf_chunks:
            return "[PDFê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.]"
        
        text = st.session_state.pdf_chunks[pdf_id]
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        keywords = re.findall(r'\w+', query.lower())
        paragraphs = text.split('\n\n')
        
        scored_paragraphs = []
        for para in paragraphs:
            if len(para.strip()) < 50:
                continue
                
            para_lower = para.lower()
            score = sum(1 for keyword in keywords if keyword in para_lower)
            
            if score > 0:
                scored_paragraphs.append((score, para.strip()))
        
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        results = []
        for i, (score, para) in enumerate(scored_paragraphs[:top_k], 1):
            if len(para) > 500:
                para = para[:500] + "..."
            results.append(f"ê°„ë‹¨ ê²€ìƒ‰ ê²°ê³¼ {i} (ê´€ë ¨ë„: {score}):\n{para}")
        
        if results:
            return "\n---\n".join(results)
        else:
            return "[ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.]"
            
    except Exception as e:
        st.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return "[ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.]"

def get_pdf_summary(pdf_id: str = "default") -> str:
    """
    PDF ìš”ì•½ ì •ë³´ ë°˜í™˜
    
    Args:
        pdf_id: PDF ì‹ë³„ì
    
    Returns:
        str: PDF ìš”ì•½ ì •ë³´
    """
    if 'pdf_chunks' not in st.session_state or pdf_id not in st.session_state.pdf_chunks:
        return "[PDF ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.]"
    
    text = st.session_state.pdf_chunks[pdf_id]
    return text[:1000] + "..." if len(text) > 1000 else text

def pdf_to_chunks(pdf_path: str, chunk_size: int = 400) -> List[str]:
    """
    PDFë¥¼ ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        chunk_size: ì²­í¬ í¬ê¸°
    
    Returns:
        List[str]: ë¶„í• ëœ ì²­í¬ë“¤
    """
    try:
        doc = fitz.open(pdf_path)
        chunks = []
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = page.get_text()
            
            # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = text.split('. ')
            
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk) + len(sentence) < chunk_size:
                    current_chunk += sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                chunks.append(current_chunk.strip())
        
        doc.close()
        return chunks
        
    except Exception as e:
        st.error(f"âŒ PDF ì²­í¬ ë¶„í•  ì˜¤ë¥˜: {e}")
        return []

def get_pdf_summary_from_session() -> str:
    """
    ì„¸ì…˜ì—ì„œ PDF ìš”ì•½ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (user_state.py í˜¸í™˜ì„±)
    
    Returns:
        str: PDF ìš”ì•½ ì •ë³´
    """
    return st.session_state.get('pdf_summary', '')

def set_pdf_summary_to_session(summary: str):
    """
    ì„¸ì…˜ì— PDF ìš”ì•½ ì •ë³´ ì„¤ì • (user_state.py í˜¸í™˜ì„±)
    
    Args:
        summary: PDF ìš”ì•½ ì •ë³´
    """
    st.session_state.pdf_summary = summary
